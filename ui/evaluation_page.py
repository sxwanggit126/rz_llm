"""
MMLUè¯„ä¼°é¡µé¢
"""
import time
import streamlit as st
import pandas as pd
from typing import List, Dict, Any
import plotly.express as px
import plotly.graph_objects as go

from ui.evaluation_api_client import get_evaluation_api_client
from ui.api_client import get_api_client


def render_evaluation_page():
    """æ¸²æŸ“è¯„ä¼°é¡µé¢"""
    st.header("ğŸ§ª MMLUæ¨¡å‹è¯„ä¼°")

    # åˆ›å»ºæ ‡ç­¾é¡µ
    tab1, tab2, tab3 = st.tabs([
        "ğŸ“‹ è¯„ä¼°é…ç½®",
        "ğŸ“Š è¯„ä¼°è¿›åº¦",
        "ğŸ“ˆ ç»“æœåˆ†æ"
    ])

    with tab1:
        render_evaluation_config()

    with tab2:
        render_evaluation_progress()

    with tab3:
        render_evaluation_results_combined()


def render_evaluation_config():
    """æ¸²æŸ“è¯„ä¼°é…ç½®é¡µé¢"""
    st.subheader("è¯„ä¼°ä»»åŠ¡é…ç½®")

    eval_client = get_evaluation_api_client()
    mmlu_client = get_api_client()

    # è·å–å·²ä¸‹è½½çš„å­¦ç§‘
    downloaded_subjects = mmlu_client.get_downloaded_subjects()

    if not downloaded_subjects:
        st.warning("è¯·å…ˆä¸‹è½½MMLUæ•°æ®åå†è¿›è¡Œè¯„ä¼°")
        st.info("å‰å¾€ã€Œæ•°æ®ä¸‹è½½ã€é¡µé¢ä¸‹è½½æ•°æ®")
        return

    # é…ç½®è¡¨å•
    with st.form("evaluation_config"):
        col1, col2 = st.columns(2)

        with col1:
            # é€‰æ‹©å­¦ç§‘
            selected_subjects = st.multiselect(
                "é€‰æ‹©è¯„ä¼°å­¦ç§‘",
                options=downloaded_subjects,
                default=["astronomy", "business_ethics"] if all(s in downloaded_subjects for s in ["astronomy", "business_ethics"]) else downloaded_subjects[:2],
                help="é€‰æ‹©è¦è¿›è¡Œè¯„ä¼°çš„å­¦ç§‘"
            )

            # æ¯ä¸ªå­¦ç§‘çš„æ•°æ®æ•°é‡
            data_count = st.slider(
                "æ¯ä¸ªå­¦ç§‘çš„æ•°æ®æ•°é‡",
                min_value=1,
                max_value=50,
                value=10,
                help="æ¯ä¸ªå­¦ç§‘éšæœºé€‰æ‹©çš„æ•°æ®æ¡æ•°"
            )

        with col2:
            # è·å–å¯ç”¨æ¨¡å‹
            available_models = eval_client.get_available_models()
            if available_models:
                selected_models = st.multiselect(
                    "é€‰æ‹©è¯„ä¼°æ¨¡å‹",
                    options=available_models,
                    default=available_models[:2] if len(available_models) >= 2 else available_models,
                    help="é€‰æ‹©è¦ä½¿ç”¨çš„è¯­è¨€æ¨¡å‹"
                )
            else:
                st.error("æ— æ³•è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨")
                return

            # è·å–Promptç±»å‹
            prompt_types = eval_client.get_prompt_types()
            if prompt_types:
                prompt_options = {pt["value"]: pt["label"] for pt in prompt_types}
                selected_prompt_types = st.multiselect(
                    "é€‰æ‹©Promptç±»å‹",
                    options=list(prompt_options.keys()),
                    default=list(prompt_options.keys()),
                    format_func=lambda x: prompt_options.get(x, x),
                    help="é€‰æ‹©è¦ä½¿ç”¨çš„Promptç­–ç•¥"
                )
            else:
                st.error("æ— æ³•è·å–Promptç±»å‹åˆ—è¡¨")
                return

        # è®¡ç®—é¢„ä¼°è¯„ä¼°æ¬¡æ•°
        if selected_subjects and selected_models and selected_prompt_types:
            total_evaluations = len(selected_subjects) * len(selected_models) * len(selected_prompt_types) * data_count
            st.info(f"é¢„ä¼°è¯„ä¼°æ¬¡æ•°: {total_evaluations} æ¬¡ (å¯èƒ½éœ€è¦ {total_evaluations * 2 // 60 + 1} åˆ†é’Ÿ)")

        # æäº¤æŒ‰é’®
        submitted = st.form_submit_button(
            "ğŸš€ å¼€å§‹è¯„ä¼°",
            type="primary",
            use_container_width=True
        )

        if submitted:
            if not selected_subjects:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªå­¦ç§‘")
            elif not selected_models:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªæ¨¡å‹")
            elif not selected_prompt_types:
                st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ç§Promptç±»å‹")
            else:
                start_evaluation_task(
                    selected_subjects,
                    selected_models,
                    selected_prompt_types,
                    data_count
                )


def start_evaluation_task(subjects: List[str], models: List[str],
                         prompt_types: List[str], data_count: int):
    """å¯åŠ¨è¯„ä¼°ä»»åŠ¡"""
    eval_client = get_evaluation_api_client()

    with st.spinner("æ­£åœ¨å¯åŠ¨è¯„ä¼°ä»»åŠ¡ï¼Œè¯·ç¨å€™..."):
        result = eval_client.start_evaluation(
            subjects=subjects,
            models=models,
            prompt_types=prompt_types,
            data_count_per_subject=data_count
        )

    if "error" in result:
        if "è¶…æ—¶" in result["error"]:
            st.warning("å¯åŠ¨è¯„ä¼°ä»»åŠ¡è¶…æ—¶ï¼Œä½†ä»»åŠ¡å¯èƒ½å·²åœ¨åå°è¿è¡Œã€‚è¯·ç¨ååœ¨ã€Œè¯„ä¼°è¿›åº¦ã€é¡µé¢æŸ¥çœ‹ã€‚")
        else:
            st.error(f"å¯åŠ¨è¯„ä¼°å¤±è´¥: {result['error']}")
        return

    # ä¿å­˜ä»»åŠ¡IDåˆ°session state
    st.session_state.current_eval_task_id = result.get("task_id")
    st.session_state.eval_subjects = subjects

    st.success(f"è¯„ä¼°ä»»åŠ¡å·²å¯åŠ¨! ä»»åŠ¡ID: {result.get('task_id')}")
    st.info("ä»»åŠ¡æ­£åœ¨åå°å¤„ç†ï¼Œè¯·å‰å¾€ã€Œè¯„ä¼°è¿›åº¦ã€é¡µé¢æŸ¥çœ‹è¿›åº¦...")


def render_evaluation_progress():
    """æ¸²æŸ“è¯„ä¼°è¿›åº¦é¡µé¢"""
    st.subheader("è¯„ä¼°è¿›åº¦ç›‘æ§")

    eval_client = get_evaluation_api_client()

    # ä»»åŠ¡IDè¾“å…¥
    col1, col2 = st.columns([3, 1])

    with col1:
        # å¦‚æœæœ‰å½“å‰ä»»åŠ¡IDï¼Œåˆ™é»˜è®¤æ˜¾ç¤º
        default_task_id = st.session_state.get("current_eval_task_id", "")
        task_id = st.text_input(
            "ä»»åŠ¡ID",
            value=default_task_id,
            placeholder="è¾“å…¥ä»»åŠ¡IDæˆ–ä»é…ç½®é¡µé¢å¯åŠ¨ä»»åŠ¡",
            help="ä»è¯„ä¼°é…ç½®é¡µé¢å¯åŠ¨ä»»åŠ¡åä¼šè‡ªåŠ¨å¡«å…¥"
        )

    with col2:
        # åˆ·æ–°æŒ‰é’®
        refresh_clicked = st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", use_container_width=True, key="refresh_eval_progress")

    if not task_id:
        st.info("è¯·è¾“å…¥ä»»åŠ¡IDæˆ–ä»ã€Œè¯„ä¼°é…ç½®ã€é¡µé¢å¯åŠ¨æ–°ä»»åŠ¡")
        return

    # è·å–ä»»åŠ¡çŠ¶æ€
    status = eval_client.get_evaluation_status(task_id)

    if "error" in status:
        st.error(f"è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥: {status['error']}")
        return

    # æ˜¾ç¤ºçŠ¶æ€ä¿¡æ¯
    display_evaluation_status(status)

    # æ ¹æ®çŠ¶æ€å†³å®šæ˜¯å¦è‡ªåŠ¨åˆ·æ–°
    task_status = status.get("status")
    if task_status in ["pending", "running"]:
        # è‡ªåŠ¨åˆ·æ–°åŠŸèƒ½ - ç¼©çŸ­åˆ·æ–°é—´éš”
        auto_refresh = st.checkbox("è‡ªåŠ¨åˆ·æ–° (5ç§’)", value=True, key="auto_refresh_progress")
        if auto_refresh:
            time.sleep(5)  # æ”¹ä¸º5ç§’åˆ·æ–°
            st.rerun()

        # æ‰‹åŠ¨åˆ·æ–°æŒ‰é’®
        if st.button("ç«‹å³åˆ·æ–°", key="manual_refresh_progress"):
            st.rerun()
    elif task_status in ["completed", "failed"]:
        # ä»»åŠ¡å·²å®Œæˆæˆ–å¤±è´¥ï¼Œä¸éœ€è¦è‡ªåŠ¨åˆ·æ–°
        st.info("ä»»åŠ¡å·²ç»“æŸï¼Œåœæ­¢è‡ªåŠ¨åˆ·æ–°")


def display_evaluation_status(status: Dict[str, Any]):
    """æ˜¾ç¤ºè¯„ä¼°çŠ¶æ€ä¿¡æ¯"""
    # åŸºæœ¬çŠ¶æ€æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)

    with col1:
        status_value = status.get("status", "unknown")
        status_color = {
            "pending": "ğŸŸ¡",
            "running": "ğŸ”µ",
            "completed": "ğŸŸ¢",
            "failed": "ğŸ”´"
        }.get(status_value, "âšª")
        st.metric("ä»»åŠ¡çŠ¶æ€", f"{status_color} {status_value}")

    with col2:
        progress = status.get("progress", 0)
        st.metric("è¿›åº¦", f"{progress:.1f}%")

    with col3:
        completed = status.get("completed_evaluations", 0)
        total = status.get("total_evaluations", 0)
        st.metric("è¯„ä¼°è¿›åº¦", f"{completed}/{total}")

    with col4:
        if status.get("created_at"):
            st.metric("åˆ›å»ºæ—¶é—´", status["created_at"][:19])

    # è¿›åº¦æ¡
    progress_value = status.get("progress", 0) / 100
    st.progress(progress_value, text=status.get("message", ""))

    # å½“å‰æ­¥éª¤
    if status.get("current_step"):
        st.info(f"å½“å‰æ­¥éª¤: {status['current_step']}")

    # æ ¹æ®çŠ¶æ€æ˜¾ç¤ºä¸åŒä¿¡æ¯
    task_status = status.get("status")

    if task_status == "completed":
        st.success("âœ… è¯„ä¼°å®Œæˆ!")
        st.balloons()

        # æ˜¾ç¤ºå®Œæˆåçš„æ“ä½œé€‰é¡¹
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“ˆ æŸ¥çœ‹ç»“æœ", type="primary", key="view_result_completed"):
                st.session_state.view_result_task_id = status.get("task_id")
                st.success("ä»»åŠ¡IDå·²è®¾ç½®ï¼Œè¯·åˆ‡æ¢åˆ°ã€Œç»“æœåˆ†æã€æ ‡ç­¾é¡µæŸ¥çœ‹è¯¦ç»†ç»“æœ")

        with col2:
            if st.button("ğŸ—‘ï¸ æ¸…é™¤ä»»åŠ¡çŠ¶æ€", key="clear_status_completed"):
                if "current_eval_task_id" in st.session_state:
                    del st.session_state.current_eval_task_id
                if "eval_subjects" in st.session_state:
                    del st.session_state.eval_subjects
                st.rerun()

    elif task_status == "failed":
        st.error("âŒ è¯„ä¼°å¤±è´¥!")
        st.error(f"é”™è¯¯ä¿¡æ¯: {status.get('message', 'æœªçŸ¥é”™è¯¯')}")

        if st.button("ğŸ—‘ï¸ æ¸…é™¤ä»»åŠ¡çŠ¶æ€", key="clear_status_failed"):
            if "current_eval_task_id" in st.session_state:
                del st.session_state.current_eval_task_id
            if "eval_subjects" in st.session_state:
                del st.session_state.eval_subjects
            st.rerun()

    elif task_status in ["pending", "running"]:
        st.info("â³ ä»»åŠ¡è¿›è¡Œä¸­...")

    # è¯¦ç»†çŠ¶æ€ä¿¡æ¯ï¼ˆå¯å±•å¼€ï¼‰
    with st.expander("è¯¦ç»†çŠ¶æ€ä¿¡æ¯", expanded=False):
        st.json(status)


def render_evaluation_results_combined():
    """æ¸²æŸ“åˆå¹¶çš„è¯„ä¼°ç»“æœåˆ†æé¡µé¢ï¼ˆåŒ…å«å†å²ä»»åŠ¡é€‰æ‹©ï¼‰"""
    st.subheader("è¯„ä¼°ç»“æœåˆ†æ")

    eval_client = get_evaluation_api_client()

    # è·å–æ‰€æœ‰å†å²ä»»åŠ¡
    with st.spinner("æ­£åœ¨åŠ è½½å†å²ä»»åŠ¡..."):
        tasks = eval_client.list_evaluation_tasks()

    if not tasks:
        st.info("æš‚æ— è¯„ä¼°ä»»åŠ¡")
        return

    # ä»»åŠ¡é€‰æ‹©ä¸‹æ‹‰æ¡†
    col1, col2 = st.columns([3, 1])

    with col1:
        # å‡†å¤‡ä»»åŠ¡é€‰é¡¹ - æ˜¾ç¤ºæ‰€æœ‰ä»»åŠ¡
        task_options = {}
        for task in tasks:
            task_id = task.get('task_id', 'Unknown')
            subjects = ', '.join(task.get('subjects', []))
            created_at = task.get('created_at', 'Unknown')[:19] if task.get('created_at') else 'Unknown'
            status = task.get('status', 'Unknown')

            # æ·»åŠ çŠ¶æ€æ ‡è¯†ç¬¦
            status_icon = {
                'completed': 'âœ…',
                'running': 'ğŸ”µ',
                'pending': 'ğŸŸ¡',
                'failed': 'âŒ'
            }.get(status, 'âšª')

            display_name = f"{status_icon} {task_id[:8]}... | {subjects} | {created_at} | {status}"
            task_options[display_name] = {'task_id': task_id, 'status': status}

        # é»˜è®¤é€‰æ‹©æœ€æ–°çš„ä»»åŠ¡
        default_index = 0
        # å¦‚æœæœ‰ä»å…¶ä»–é¡µé¢ä¼ è¿‡æ¥çš„ä»»åŠ¡IDï¼Œåˆ™é€‰ä¸­å®ƒ
        default_task_id = st.session_state.get("view_result_task_id")
        if default_task_id:
            for i, (display_name, task_info) in enumerate(task_options.items()):
                if task_info['task_id'] == default_task_id:
                    default_index = i
                    break

        selected_task_display = st.selectbox(
            "é€‰æ‹©è¦æŸ¥çœ‹çš„è¯„ä¼°ä»»åŠ¡",
            options=list(task_options.keys()),
            index=default_index,
            help="é€‰æ‹©ä¸€ä¸ªè¯„ä¼°ä»»åŠ¡æ¥æŸ¥çœ‹è¯¦ç»†ä¿¡æ¯ï¼ˆâœ…=å·²å®Œæˆï¼ŒğŸ”µ=è¿è¡Œä¸­ï¼ŒğŸŸ¡=ç­‰å¾…ä¸­ï¼ŒâŒ=å¤±è´¥ï¼‰"
        )

        selected_task_info = task_options[selected_task_display]
        selected_task_id = selected_task_info['task_id']
        selected_task_status = selected_task_info['status']

    with col2:
        col2_1, col2_2 = st.columns(2)
        with col2_1:
            if st.button("ğŸ”„ åˆ·æ–°ä»»åŠ¡åˆ—è¡¨", use_container_width=True, key="refresh_task_list"):
                st.cache_data.clear()
                st.rerun()
        with col2_2:
            if st.button("ğŸ’¾ å¼ºåˆ¶åŒæ­¥çŠ¶æ€", use_container_width=True, key="force_sync_status",
                        help="å¦‚æœä»»åŠ¡å®é™…å·²å®Œæˆä½†æ˜¾ç¤ºpendingï¼Œç‚¹å‡»æ­¤æŒ‰é’®"):
                # å¼ºåˆ¶æ¸…é™¤æ‰€æœ‰ç¼“å­˜å¹¶é‡æ–°åŠ è½½
                with st.spinner("æ­£åœ¨å¼ºåˆ¶åŒæ­¥çŠ¶æ€..."):
                    # è¿™é‡Œå¯ä»¥è°ƒç”¨æ¸…é™¤ç¼“å­˜çš„API
                    try:
                        # æ¸…é™¤æœ¬åœ°ç¼“å­˜
                        st.cache_data.clear()
                        st.cache_resource.clear()
                        st.success("ç¼“å­˜å·²æ¸…é™¤ï¼Œé¡µé¢å°†é‡æ–°åŠ è½½æœ€æ–°çŠ¶æ€")
                        st.rerun()
                    except Exception as e:
                        st.error(f"åŒæ­¥å¤±è´¥: {e}")

    # æ˜¾ç¤ºé€‰ä¸­ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯
    selected_task_info_detail = next((task for task in tasks if task.get('task_id') == selected_task_id), None)

    if selected_task_info_detail:
        with st.expander("ğŸ“‹ ä»»åŠ¡è¯¦ç»†ä¿¡æ¯", expanded=False):
            col1, col2 = st.columns(2)

            with col1:
                st.write(f"**ä»»åŠ¡ID:** {selected_task_info_detail.get('task_id', 'Unknown')}")
                st.write(f"**çŠ¶æ€:** {selected_task_info_detail.get('status', 'Unknown')}")
                st.write(f"**å­¦ç§‘:** {', '.join(selected_task_info_detail.get('subjects', []))}")
                st.write(f"**æ¨¡å‹:** {', '.join(selected_task_info_detail.get('models', []))}")

            with col2:
                st.write(f"**Promptç±»å‹:** {', '.join(selected_task_info_detail.get('prompt_types', []))}")
                st.write(f"**æ¯å­¦ç§‘æ•°æ®é‡:** {selected_task_info_detail.get('data_count_per_subject', 'Unknown')}")
                st.write(f"**åˆ›å»ºæ—¶é—´:** {selected_task_info_detail.get('created_at', 'Unknown')}")
                st.write(f"**æ›´æ–°æ—¶é—´:** {selected_task_info_detail.get('updated_at', 'Unknown')}")

    st.divider()

    # æ ¹æ®ä»»åŠ¡çŠ¶æ€æ˜¾ç¤ºä¸åŒå†…å®¹
    if selected_task_status == 'completed':
        # å·²å®Œæˆçš„ä»»åŠ¡ï¼Œæ˜¾ç¤ºå®Œæ•´çš„ç»“æœåˆ†æ
        load_and_display_results(selected_task_id)
    elif selected_task_status == 'running':
        st.info("ğŸ“ è¯¥ä»»åŠ¡æ­£åœ¨è¿è¡Œä¸­ï¼Œè¯·å‰å¾€ã€Œè¯„ä¼°è¿›åº¦ã€é¡µé¢æŸ¥çœ‹å®æ—¶è¿›åº¦")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“Š æŸ¥çœ‹è¿›åº¦", key="goto_progress_running"):
                st.session_state.current_eval_task_id = selected_task_id
                st.success("ä»»åŠ¡IDå·²è®¾ç½®ï¼Œè¯·åˆ‡æ¢åˆ°ã€Œè¯„ä¼°è¿›åº¦ã€æ ‡ç­¾é¡µæŸ¥çœ‹")
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_running_status"):
                st.rerun()
    elif selected_task_status == 'pending':
        st.warning("â³ è¯¥ä»»åŠ¡æ­£åœ¨ç­‰å¾…æ‰§è¡Œï¼Œè¯·å‰å¾€ã€Œè¯„ä¼°è¿›åº¦ã€é¡µé¢æŸ¥çœ‹")
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸ“Š æŸ¥çœ‹è¿›åº¦", key="goto_progress_pending"):
                st.session_state.current_eval_task_id = selected_task_id
                st.success("ä»»åŠ¡IDå·²è®¾ç½®ï¼Œè¯·åˆ‡æ¢åˆ°ã€Œè¯„ä¼°è¿›åº¦ã€æ ‡ç­¾é¡µæŸ¥çœ‹")
        with col2:
            if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_pending_status"):
                st.rerun()
    elif selected_task_status == 'failed':
        st.error("âŒ è¯¥ä»»åŠ¡å·²å¤±è´¥ï¼Œæ— æ³•æŸ¥çœ‹å®Œæ•´ç»“æœ")

        # å°è¯•åŠ è½½éƒ¨åˆ†ç»“æœ
        with st.expander("ğŸ” å°è¯•æŸ¥çœ‹éƒ¨åˆ†ç»“æœ", expanded=False):
            st.warning("ä»»åŠ¡å¤±è´¥ï¼Œä½†å¯èƒ½å­˜åœ¨éƒ¨åˆ†è¯„ä¼°ç»“æœ")
            try:
                load_and_display_results(selected_task_id)
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½ç»“æœ: {str(e)}")
    else:
        st.warning(f"âšª æœªçŸ¥ä»»åŠ¡çŠ¶æ€: {selected_task_status}")
        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", key="refresh_unknown_status"):
            st.rerun()


def load_and_display_results(task_id: str):
    """åŠ è½½å¹¶æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
    eval_client = get_evaluation_api_client()

    # è·å–æ±‡æ€»ç»“æœ
    results = eval_client.get_evaluation_results(task_id)

    if "error" in results:
        st.error(f"è·å–è¯„ä¼°ç»“æœå¤±è´¥: {results['error']}")
        return

    # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
    display_overall_stats(results.get("overall_stats", {}))

    st.divider()

    # æ˜¾ç¤ºæ±‡æ€»ç»“æœè¡¨æ ¼
    display_summary_table(results.get("summaries", []))

    st.divider()

    # æ˜¾ç¤ºå¯è§†åŒ–å›¾è¡¨
    display_result_charts(results.get("raw_summaries", []))

    st.divider()

    # è¯¦ç»†ç»“æœé€‰é¡¹
    if st.button("ğŸ“‹ æŸ¥çœ‹è¯¦ç»†ç»“æœ", key=f"view_detailed_results_{task_id[:8]}"):
        display_detailed_results(task_id)


def display_overall_stats(stats: Dict[str, Any]):
    """æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
    st.subheader("ğŸ“Š æ€»ä½“ç»Ÿè®¡")

    col1, col2, col3, col4 = st.columns(4)

    with col1:
        st.metric("æ€»è¯„ä¼°æ¬¡æ•°", stats.get("total_evaluations", 0))

    with col2:
        st.metric("æ­£ç¡®æ¬¡æ•°", stats.get("total_correct", 0))

    with col3:
        accuracy = stats.get("overall_accuracy", 0)
        st.metric("æ€»ä½“æ­£ç¡®ç‡", f"{accuracy:.2%}")

    with col4:
        st.metric("è¯„ä¼°ç»„åˆæ•°", f"{stats.get('models_count', 0)}Ã—{stats.get('prompt_types_count', 0)}")


def display_summary_table(summaries: List[Dict[str, Any]]):
    """æ˜¾ç¤ºæ±‡æ€»ç»“æœè¡¨æ ¼"""
    st.subheader("ğŸ“ˆ ç»“æœæ±‡æ€»è¡¨")

    if not summaries:
        st.warning("æš‚æ— æ±‡æ€»ç»“æœ")
        return

    # è½¬æ¢ä¸ºDataFrameå¹¶æŒ‰æ¨¡å‹åç§°æ’åº
    df = pd.DataFrame(summaries)
    df = df.sort_values('æ¨¡å‹åç§°')

    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(
        df,
        use_container_width=True,
        hide_index=True,
        column_config={
            "æ¨¡å‹åç§°": st.column_config.TextColumn("æ¨¡å‹åç§°", width="medium"),
            "æ–¹å¼": st.column_config.TextColumn("Promptæ–¹å¼", width="medium"),
            "æ­£ç¡®ç‡": st.column_config.TextColumn("æ­£ç¡®ç‡", width="small"),
            "æ­£ç¡®æ•°": st.column_config.NumberColumn("æ­£ç¡®æ•°", width="small"),
            "æ€»æ•°": st.column_config.NumberColumn("æ€»æ•°", width="small"),
        }
    )


def display_result_charts(raw_summaries: List[Dict[str, Any]]):
    """æ˜¾ç¤ºç»“æœå¯è§†åŒ–å›¾è¡¨ - ä»…æŸ±çŠ¶å›¾"""
    st.subheader("ğŸ“Š ç»“æœå¯è§†åŒ–")

    if not raw_summaries:
        st.warning("æš‚æ— æ•°æ®å¯è§†åŒ–")
        return

    # å‡†å¤‡æ•°æ®
    df = pd.DataFrame(raw_summaries)

    # æŒ‰æ¨¡å‹å’ŒPromptç±»å‹åˆ†ç»„çš„æŸ±çŠ¶å›¾ - å¹¶æ’æ˜¾ç¤º
    fig = px.bar(
        df,
        x="model_name",
        y="accuracy",
        color="prompt_type",
        title="å„æ¨¡å‹åœ¨ä¸åŒPromptç­–ç•¥ä¸‹çš„æ­£ç¡®ç‡",
        labels={"accuracy": "æ­£ç¡®ç‡", "model_name": "æ¨¡å‹", "prompt_type": "Promptç±»å‹"},
        height=500,
        barmode='group'  # å…³é”®å‚æ•°ï¼šå¹¶æ’æ˜¾ç¤ºè€Œä¸æ˜¯å †å 
    )
    fig.update_layout(yaxis_tickformat='.2%')
    st.plotly_chart(fig, use_container_width=True)


def display_detailed_results(task_id: str):
    """æ˜¾ç¤ºè¯¦ç»†è¯„ä¼°ç»“æœ"""
    st.subheader("ğŸ“‹ è¯¦ç»†è¯„ä¼°ç»“æœ")

    eval_client = get_evaluation_api_client()

    with st.spinner("æ­£åœ¨åŠ è½½è¯¦ç»†ç»“æœ..."):
        details = eval_client.get_evaluation_details(task_id)

    if "error" in details:
        st.error(f"è·å–è¯¦ç»†ç»“æœå¤±è´¥: {details['error']}")
        return

    detail_items = details.get("details", [])

    if not detail_items:
        st.warning("æš‚æ— è¯¦ç»†ç»“æœ")
        return

    st.info(f"å…± {len(detail_items)} æ¡è¯¦ç»†è¯„ä¼°è®°å½•")

    # è¿‡æ»¤é€‰é¡¹
    col1, col2, col3 = st.columns(3)

    df_details = pd.DataFrame(detail_items)

    with col1:
        # æŒ‰å­¦ç§‘è¿‡æ»¤
        subjects = ["å…¨éƒ¨"] + list(df_details["subject"].unique())
        selected_subject = st.selectbox("æŒ‰å­¦ç§‘è¿‡æ»¤", subjects)

    with col2:
        # æŒ‰æ¨¡å‹è¿‡æ»¤
        models = ["å…¨éƒ¨"] + list(df_details["model_name"].unique())
        selected_model = st.selectbox("æŒ‰æ¨¡å‹è¿‡æ»¤", models)

    with col3:
        # æŒ‰ç»“æœè¿‡æ»¤
        result_filter = st.selectbox("æŒ‰ç»“æœè¿‡æ»¤", ["å…¨éƒ¨", "æ­£ç¡®", "é”™è¯¯"])

    # åº”ç”¨è¿‡æ»¤
    filtered_df = df_details.copy()

    if selected_subject != "å…¨éƒ¨":
        filtered_df = filtered_df[filtered_df["subject"] == selected_subject]

    if selected_model != "å…¨éƒ¨":
        filtered_df = filtered_df[filtered_df["model_name"] == selected_model]

    if result_filter == "æ­£ç¡®":
        filtered_df = filtered_df[filtered_df["is_correct"] == True]
    elif result_filter == "é”™è¯¯":
        filtered_df = filtered_df[filtered_df["is_correct"] == False]

    # æ˜¾ç¤ºè¿‡æ»¤åçš„ç»“æœ
    st.write(f"è¿‡æ»¤å: {len(filtered_df)} æ¡è®°å½•")

    # åˆ†é¡µæ˜¾ç¤º
    page_size = 10
    total_pages = (len(filtered_df) + page_size - 1) // page_size

    if total_pages > 1:
        page = st.selectbox("é¡µç ", range(1, total_pages + 1))
        start_idx = (page - 1) * page_size
        end_idx = start_idx + page_size
        page_df = filtered_df.iloc[start_idx:end_idx]
    else:
        page_df = filtered_df

    # æ˜¾ç¤ºè¯¦ç»†è®°å½•
    for idx, row in page_df.iterrows():
        with st.expander(f"è®°å½• {idx + 1}: {row['subject']} - {row['model_name']} - {row['prompt_type']} {'âœ…' if row['is_correct'] else 'âŒ'}"):
            col1, col2 = st.columns(2)

            with col1:
                st.write("**åŸºæœ¬ä¿¡æ¯:**")
                st.write(f"- å­¦ç§‘: {row['subject']}")
                st.write(f"- æ¨¡å‹: {row['model_name']}")
                st.write(f"- Promptç±»å‹: {row['prompt_type']}")
                st.write(f"- é¢˜ç›®åºå·: {row['question_index']}")

            with col2:
                st.write("**è¯„ä¼°ç»“æœ:**")
                st.write(f"- é¢„æµ‹ç­”æ¡ˆ: {row['predicted_answer']}")
                st.write(f"- æ­£ç¡®ç­”æ¡ˆ: {row['correct_answer']}")
                st.write(f"- æ˜¯å¦æ­£ç¡®: {'âœ… æ˜¯' if row['is_correct'] else 'âŒ å¦'}")
                st.write(f"- è¯„ä¼°æ—¶é—´: {row['evaluation_time']}")

            st.write("**æ¨¡å‹å›å¤:**")
            st.text_area("", value=row['response_content'], height=100, disabled=True, key=f"response_{idx}_{row['question_index']}")